正則化
正則化（せいそくか、英: regularization）とは、数学・統計学において、特に機械学習と逆問題でよく使われるが、機械学習で過学習を防いだり、逆問題での不良設定問題を解くために、追加の項を導入する手法である。モデルの複雑さに罰則を科すために導入され、なめらかでない事に罰則をかけたり、パラメータのノルムの大きさに罰則をかけたりする。
正則化の理論的正当化はオッカムの剃刀にある。ベイジアンの観点では、多くの正則化の手法は、モデルのパラメータの事前情報にあたる。
統計および機械学習における正則化[編集]
統計および機械学習において、正則化はモデルのパラメータの学習に使われ、特に過学習を防ぎ、汎化能力を高めるために使われる。
機械学習において最も一般的なのは L1 正則化 (p=1) と L2 正則化 (p=2) である。損失関数
E
(
w
)
E(\mathbf{w}) の代わりに、
E
(
w
)
+
λ
1
p
∥
w
∥
p
p
=
E
(
w
)
+
λ
1
p
∑
i
|
w
i
|
p
E(\mathbf{w}) + \lambda \frac{1}{p} \| \mathbf{w} \|_p^p = E(\mathbf{w}) + \lambda \frac{1}{p} \sum_i |w_i|^p
を使用する。
w
\mathbf{w} はパラメータのベクトルで、
∥
⋅
∥
p
\| \cdot \|_p は L1 ノルム (p=1) や L2 ノルム (p=2) などである。
λ\lambda  はハイパーパラメータで、正の定数で、大きくするほど正則化の効果が強くなるが、交差確認などで決める。
損失関数をパラメータで偏微分すると、
L2 正則化の場合
∂
E
(
w
)
∂
w
i
+
λ
w
i
\frac{ \partial E(\mathbf{w}) }{ \partial w_i } + \lambda w_i
L1 正則化の場合
∂
E
(
w
)
∂
w
i
+
λ
sgn
⁡
(
w
i
)
\frac{ \partial E(\mathbf{w}) }{ \partial w_i } + \lambda \sgn(w_i)
となり、これは、最急降下法や確率的勾配降下法を使用する場合は、L2 正則化はパラメータの大きさに比例した分だけ、L1 正則化は
λ\lambda  だけ 0 に近づける事を意味する。
この手法は様々なモデルで利用できる。線形回帰モデルに利用した場合は、L1 の場合は Lasso[1]、L2 の場合はリッジ回帰[2]と呼ぶ。ロジスティック回帰、ニューラルネットワーク、サポートベクターマシン、条件付き確率場 などでも使われる。ニューラルネットワークの世界では、L2 正則化は荷重減衰（英: weight decay）とも呼ばれる。
